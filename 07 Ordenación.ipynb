{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Ordenación\n",
    "\n",
    "El problema de ordenar un conjunto de datos (por ejemplo, en orden ascendente) tiene gran importancia tanto teórica como práctica. En esta sección veremos principalmente algoritmos que ordenan mediante comparaciones entre llaves, para los cuales se puede demostrar una cota inferior que coincide con la cota superior provista por varios algoritmos. También veremos un algoritmo de otro tipo, que al no hacer comparaciones, no está sujeto a esa cota inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cota inferior\n",
    "\n",
    "Supongamos que deseamos ordenar tres datos $A$, $B$ y $C$. La siguiente figura muestra un árbol de decisión posible para resolver este problema. Los nodos internos del árbol representan comparaciones y los nodos externos representan salidas emitidas por el programa.\n",
    "\n",
    "![arbol-decision-ordenacion](arbol-decision-ordenacion.gif)\n",
    "\n",
    "Como se vio en el capítulo de búsqueda, todo árbol de decisión con $H$ hojas tiene al menos altura $\\log_2{H}$, y la altura del árbol de decisión es igual al número de comparaciones que se efectúan en el peor caso.\n",
    "\n",
    "En un árbol de decisión para ordenar $n$ datos se tiene que $H=n!$, y por lo tanto se tiene que todo algoritmo que ordene $n$ datos mediante comparaciones entre llaves debe hacer al menos $\\log_2{n!}$ comparaciones en el peor caso.\n",
    "\n",
    "Usando la aproximación de Stirling, se puede demostrar que $\\log_2{n!} = n \\log_2{n} + \\Theta(n)$, por lo cual la cota inferior es de $\\Theta(n\\log{n})$.\n",
    "\n",
    "Si suponemos que todas las posibles permutaciones resultantes son equiprobables, es posible demostrar también que el número promedio de comparaciones que cualquier algoritmo debe hacer es también de $\\Theta(n\\log{n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quicksort\n",
    "\n",
    "Este método fue inventado por C.A.R. Hoare a comienzos de los '60s, y sigue siendo el método más eficiente para uso general.\n",
    "\n",
    "Quicksort es un ejemplo clásico de la aplicación del principio de *dividir para reinar*. Su estructura es la siguiente:\n",
    "\n",
    "* Primero se elige un elemento al azar, que se denomina el pivote.\n",
    "\n",
    "* El arreglo a ordenar se reordena dejando a la izquierda a los elementos menores que el pivote, el pivote al medio, y a la derecha los elementos mayores que el pivote:\n",
    "\n",
    "![particion](particion.gif)\n",
    "\n",
    "* Luego cada sub-arreglo se ordena recursivamente.\n",
    "\n",
    "La recursividad termina, en principio, cuando se llega a sub-arreglos de tamaño cero o uno, los cuales trivialmente ya están ordenados. En la práctica veremos que es preferible detener la recursividad antes de eso, para no desperdiciar tiempo ordenando recursivamente arreglos pequeños, los cuales pueden ordenarse más eficientemente usando Ordenación por Inserción, por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quicksort(a):\n",
    "    qsort(a,0,len(a)-1)\n",
    "\n",
    "def qsort(a,i,j): # ordena a[i],...,a[j]\n",
    "    if i<j: # quedan 2 o más elementos por ordenar\n",
    "        k=particion(a,i,j)\n",
    "        qsort(a,i,k-1)\n",
    "        qsort(a,k+1,j)\n",
    "\n",
    "def particion(a,i,j): # particiona a[i],...,a[j], retorna en k posición del pivote\n",
    "    k=np.random.randint(i,j) # genera un número al azar k en i..j\n",
    "    (a[i],a[k])=(a[k],a[i]) # mueve a[k] al extremo izquierdo\n",
    "    # a[i] es el pivote\n",
    "    s=i # invariante: a[i+1..s]<=a[i], a[s+1..t]>a[i]\n",
    "    for t in range(s,j):\n",
    "        if a[t+1]<=a[i]:\n",
    "            (a[s+1],a[t+1])=(a[t+1],a[s+1])\n",
    "            s=s+1\n",
    "    # mover pivote al centro\n",
    "    (a[i],a[s])=(a[s],a[i])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37316239 0.90264899 0.40869601 0.2787652  0.55497464 0.90363899]\n",
      "[0.2787652  0.37316239 0.40869601 0.55497464 0.90264899 0.90363899]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "quicksort(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costo promedio de Quicksort\n",
    "\n",
    "Si suponemos, como una primera aproximación, que el pivote siempre resulta ser la mediana del conjunto, entonces el costo de ordenar está dado (aproximadamente) por la ecuación de recurrencia\n",
    "\n",
    "$$\n",
    "T(n)=n+2T\\left( \\frac{n}{2} \\right)\n",
    "$$\n",
    "\n",
    "Esto tiene solución $T(n) = n \\log_2{n}$ y es, en realidad, el *mejor* caso de Quicksort.\n",
    "\n",
    "Para analizar el tiempo promedio que demora la ordenación mediante Quicksort, observemos que el funcionamiento de Quicksort puede graficarse mediante un *árbol de partición*:\n",
    "\n",
    "![arbol-particion](arbol-particion.gif)\n",
    "\n",
    "Por la forma en que se construye, es fácil ver que el árbol de partición es un *árbol de búsqueda binaria*, y como el pivote es escogido al azar, entonces la raíz de cada subárbol puede ser cualquiera de los elementos del conjunto en forma equiprobable. En consecuencia, los árboles de partición y los árboles de búsqueda binaria tienen exactamente la misma distribución.\n",
    "\n",
    "En el proceso de partición, cada elemento de los subárboles ha sido comparado contra la raíz (el pivote). Al terminar el proceso, cada elemento ha sido comparado contra todos sus ancestros. Si sumamos todas estas comparaciones, el resultado total es igual al *largo de caminos internos*.\n",
    "\n",
    "Usando todas estas correspondencias, tenemos que, usando los resultados ya conocidos para árboles, el número promedio de comparaciones que realiza Quicksort es de:\n",
    "\n",
    "$$\n",
    "T(n)=1.38 n\\log_2{n}+\\Theta(n)\n",
    "$$\n",
    "\n",
    "Por lo tanto, Quicksort, en el caso esperado, corre en un tiempo proporcional a la cota inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peor caso de Quicksort\n",
    "\n",
    "El peor caso de Quicksort se produce cuando el pivote resulta ser siempre el mínimo o el máximo del conjunto. En este caso la ecuación de recurrencia es\n",
    "\n",
    "$$\n",
    "T(n) = n - 1 + T(n-1)\n",
    "$$\n",
    "            \n",
    "lo que tiene solución $T(n) = \\Theta(n^2)$. Desde el punto de vista del árbol de partición, esto corresponde a un árbol en \"zig-zag\".\n",
    "\n",
    "Si bien este peor caso es extremadamente improbable si el pivote se escoge al azar, algunas implementaciones de Quicksort toman como pivote al primer elemento del arreglo (suponiendo que, al venir el arreglo al azar, entonces el primer elemento es tan aleatorio como cualquier otro). El problema es que si el conjunto viene en realidad ordenado, entonces caemos justo en el peor caso cuadrático.\n",
    "\n",
    "Lo anterior refuerza la importancia de que el pivote se escoja al azar. Esto no aumenta significativamente el costo total, porque el número total de elecciones de pivote es $\\Theta(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejoras a Quicksort\n",
    "\n",
    "Quicksort puede ser optimizado de varias maneras, pero hay que ser muy cuidadoso con estas mejoras, porque es fácil que terminen empeorando el desempeño del algoritmo.\n",
    "\n",
    "En primer lugar, es desaconsejable hacer cosas que aumenten la cantidad de trabajo que se hace dentro del \"loop\" de partición, porque este es el lugar en donde se concentra el costo $\\Theta(n \\log{n})$.\n",
    "\n",
    "Algunas de las mejoras que han dado buen resultado son las siguientes:\n",
    "\n",
    "#### Quicksort con \"mediana de 3\"\n",
    "\n",
    "En esta variante, el pivote no se escoge como un elemento tomado al azar, sino que primero se extrae una muestra de 3 elementos, y entre ellos se escoge a la mediana de esa muestra como pivote.\n",
    "\n",
    "Si la muestra se escoge tomando al primer elemento del arreglo, al del medio y al último, entonces lo que era el peor caso (arreglo ordenado) se transforma de inmediato en mejor caso.\n",
    "\n",
    "De todas formas, es aconsejable que la muestra se escoja al azar, y en ese caso el análisis muestra que el costo esperado para ordenar n elementos es\n",
    "\n",
    "$$\n",
    "\\frac{12}{7} n \\ln{n}  \\approx  1.19 n \\log_2{n}\n",
    "$$\n",
    "\n",
    "Esta reducción en el costo se debe a que el pivote es ahora una mejor aproximación a la mediana. De hecho, si en lugar de escoger una muestra de tamaño 3, lo hiciéramos con tamaños como 7, 9, etc., se lograría una reducción aún mayor, acercándonos cada vez más al óptimo, pero con rendimientos rápidamente decrecientes.\n",
    "\n",
    "#### Uso de Ordenación por Inserción para ordenar sub-arreglos pequeños\n",
    "\n",
    "Tal como se dijo antes, no es eficiente ordenar recursivamente sub-arreglos demasiado pequeños.\n",
    "\n",
    "En lugar de esto, se puede establecer un tamaño mínimo $M$, de modo que los sub-arreglos de tamaño menor que esto se ordenan por inserción en lugar de por Quicksort.\n",
    "\n",
    "Claramente debe haber un valor óptimo para $M$, porque si creciera indefinidamente se llegaría a un algoritmo cuadrático. Esto se puede analizar, y el óptimo es cercano a 10.\n",
    "\n",
    "Como método de implementación, al detectarse un sub-arreglo de tamaño menor que $M$, se lo puede dejar simplemente sin ordenar, retornando de inmediato de la recursividad. Al final del proceso, se tiene un arreglo cuyos pivotes están en orden creciente, y encierran entre ellos a bloques de elementos desordenados, pero que ya están en el grupo correcto. Para completar la ordenación, entonces, basta con hacer una sola gran pasada de Ordenación por Inserción, la cual ahora no tiene costo $\\Theta(n^2)$, sino $\\Theta(nM)$, porque ningún elemento está a distancia mayor que $M$ de su ubicación definitiva.\n",
    "\n",
    "#### Ordenar recursivamente sólo el sub-arreglo más pequeño\n",
    "\n",
    "Un problema potencial con Quicksort es la profundidad que puede llegar a tener la recursividad. En el peor caso, ésta puede llegar a ser $\\Theta(n)$.\n",
    "\n",
    "Para evitar esto, se puede usar recursividad solo para la \"mitad\" más pequeña, y ordenar la otra \"mitad\" de manera iterativa. Con este enfoque, cada llamada recursiva se aplica a un sub-arreglo cuyo tamaño es a lo más la mitad del tamaño del arreglo a ordenar, de modo que si llamamos $S(n)$ a la profundidad de recursión, tenemos que\n",
    "\n",
    "$$\n",
    "S(n) \\le 1 + S\\left(\\frac{n}{2}\\right)\n",
    "$$\n",
    "\n",
    "lo cual tiene solución $\\log_2{n}$, de modo que la profundidad de la recursión nunca es más que logarítmica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsort(a,i,j): # ordena a[i],...,a[j]\n",
    "    while i<j: # quedan 2 o más elementos por ordenar\n",
    "        k=particion(a,i,j)\n",
    "        if k-i<=j-k: #mitad izquierda es más pequeña\n",
    "            qsort(a,i,k-1)\n",
    "            i=k+1\n",
    "        else:\n",
    "            qsort(a,k+1,j)\n",
    "            j=k-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66656624 0.76798384 0.87811887 0.74763235 0.30180187 0.82056957]\n",
      "[0.30180187 0.66656624 0.74763235 0.76798384 0.82056957 0.87811887]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "quicksort(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un algoritmo de selección basado en Quicksort\n",
    "\n",
    "Es posible modificar el algoritmo de Quicksort para seleccionar el $k$-ésimo elemento de un arreglo $a[0],\\ldots,a[n-1]$, esto es, el elemento que quedaría en la posición $a[k]$ si el arreglo se ordenara. Una solución trivial sería, precisamente, ordenar el arreglo y retornar ese elemento, pero la pregunta es si eso se puede hacer de manera más eficiente.\n",
    "\n",
    "Una idea para resolver este problema idea es ejecutar Quicksort, pero en lugar de ordenar las dos mitades, hacerlo solo con aquella mitad en donde se encontraría el elemento buscado, o incluso no hacer nada si tenemos suerte y el pivote quedó justo en la posición $a[k]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickselect(a,k):\n",
    "    assert k>=0 and k<len(a)\n",
    "    qselect(a,k,0,len(a)-1)\n",
    "    return a[k]\n",
    "\n",
    "def qselect(a,k,i,j): # selecciona el elemento que quedaría en a[k]\n",
    "                      # si se ordenara a[i],...,a[j] (i<=k<=j)\n",
    "    if i<j:\n",
    "        p=particion(a,i,j)\n",
    "        if p!=k:\n",
    "            if k<p:\n",
    "                qselect(a,k,i,p-1)\n",
    "            else:\n",
    "                qselect(a,k,p+1,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19052461 0.578469   0.14702632 0.62380414 0.21373454 0.59978106]\n",
      "k=4\n",
      "0.5997810643996624\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "k=int(input(\"k=\"))\n",
    "print(quickselect(a,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad la recursividad de ``qselect`` es fácil de transformar en iteración, y una vez hecho eso ya no hay razón para que sea una función aparte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickselect(a,k):\n",
    "    assert k>=0 and k<len(a)\n",
    "    i=0\n",
    "    j=len(a)-1\n",
    "    while i<j:\n",
    "        p=particion(a,i,j)\n",
    "        if p==k:\n",
    "            break\n",
    "        if k<p:\n",
    "            j=p-1\n",
    "        else:\n",
    "            i=p+1\n",
    "    return a[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28147833 0.06014974 0.56597127 0.42582274 0.66869136 0.11621771]\n",
      "k=3\n",
      "0.4258227414342961\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random(6)\n",
    "print(a)\n",
    "k=int(input(\"k=\"))\n",
    "print(quickselect(a,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis del costo esperado de Quickselect es complicado, pero se puede demostrar que es $\\Theta(n)$. Por otra parte, el peor caso se da cuando el pivote cae siempre en un extremo, y en ese caso el costo es $\\Theta(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un algoritmo de selección de tiempo lineal en el peor caso\n",
    "\n",
    "La razón por la cual el algoritmo ``Quickselect`` puede demorar tiempo cuadrático en el peor caso es que el pivote puede resultar ser siempre el mínimo o el máximo, o un elemento muy cercano a los extremos.\n",
    "\n",
    "Esta debilidad se subsanaría si pudiéramos garantizar que los elementos que quedan a cada lado del pivote son siempre al menos una cierta fracción fija del total, digamos $\\alpha n$, para algún $\\alpha \\in (0,\\frac12)$.\n",
    "\n",
    "Una forma de lograrlo es la siguiente. Dividamos el conunto completo de $n$ elementos en grupos de tamaño $5$, y encontremos la mediana de cada uno de estos pequeños conjuntos. Cada uno de ellos se puede visualizar como un orden parcial de la forma\n",
    "\n",
    "![spider](spider.png)\n",
    "\n",
    "donde los dos elementos de arriba son mayores que la mediana, y ésta es mayor que los dos de abajo.\n",
    "\n",
    "A continuación, aplicamos este mismo algoritmo, recursivamente, para encontrar la **mediana de las medianas**. Este elemento, que se muestra encerrado en un círculo en el diagrama siguiente, será nuestro pivote:\n",
    "\n",
    "![median-of-medians](median-of-medians.png)\n",
    "\n",
    "Como se ve, todos los elementos encerrados en las líneas punteadas están garantizados de estar por sobre o por debajo del pivote, respectivamente.\n",
    "Por lo tanto, en el peor caso, podemos garantizar que al menos $\\frac{3}{10}n$ elementos son menores que el pivote, otros $\\frac{3}{10}n$ son mayores que él.\n",
    "\n",
    "Por lo tanto, en el peor caso, la llamada recursiva que se hace para continuar el proceso se aplica a un conjunto de a lo más $\\frac{7}{10}n$ elementos.\n",
    "\n",
    "Si llamamos $T(n)$ al tiempo que demora este algoritmo en el peor caso, y si $cn$ es el tiempo que demora encontrar las medianas de los $n/5$ subconjuntos, para alguna constante $c$, entonces tenemos que, en el peor caso,\n",
    "\n",
    "$$\n",
    "T(n)=cn+T\\left(\\frac{n}{5}\\right)+T\\left(\\frac{7n}{10}\\right)\n",
    "$$\n",
    "\n",
    "Esta ecuación tiene solución lineal. En efecto, supongamos que existe una constante $d$ tal que $T(n)=dn$. Reemplazando en la ecuación, tenemos\n",
    "\n",
    "$$\n",
    "dn=cn+\\frac{dn}{5}+\\frac{7dn}{10}\n",
    "$$\n",
    "\n",
    "de donde podemos despejar $d$ y obtener $d=10c$. Por lo tanto, $T(n)=10cn$, y el algoritmo demora tiempo $Theta(n)$ en el peor caso.\n",
    "\n",
    "La razón por la cual la ecuación admite solución lineal es porque los coeficientes de $n$ en las llamadas recursivas suman $\\frac{1}{5}+\\frac{7}{10}<1$. Eso explica por qué no habría funcionado haber dividido el conjunto en grupos de tamaño 3. En ese caso, los coeficientes habrían sido $\\frac{1}{3}$ y $\\frac{2}{3}$ respectivamente, y su suma no sería menor que $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
